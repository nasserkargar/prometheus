# nano docker-compose.yml
version: '3'
services:
  prometheus:
    image: prom/prometheus
    ports:
      - '9090:9090'
    volumes:
      - ./prometheus:/etc/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--web.enable-lifecycle'
    restart: always
  
  grafana:
    image: grafana/grafana
    ports:
      - '3000:3000'
    depends_on:
      - prometheus
    restart: always

  alertmanager:
    image: prom/alertmanager
    ports:
      - '9093:9093'
    volumes:
      - ./alertmanager:/etc/alertmanager
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
    restart: always
  
  
  web_server:
    image: httpd:2.4
    ports:
      - '80:80'
    restart: always

# mkdir prometheus
# nano ./prometheus/prometheus.yml
global:
  scrape_interval: 15s
scrape_configs:
  - job_name: 'apache'
    static_configs:
      - targets: ['web_server:80']


# mkdir alertmanager
# nano ./alertmanager/alertmanager.yml
global:
  resolve_timeout: 5m
route:
  group_by: ['alertname', 'severity']
  group_wait: 10s
  group_interval: 5m
  repeat_interval: 3h
receivers:
- name: 'email-notification'
  email_configs:
  - to: 'youremail@example.com'
    send_resolved: true
inhibit_rules:
- source_match:
    severity: 'critical'
  target_match:
    severity: 'warning'
  equal: ['alertname', 'instance']

# nano ./alertmanager/alerts.rules.yml
groups:
- name: example_alerts
  rules:
  - alert: HighRequestRate
    expr: rate(http_requests_total{job="web_server"}[5m]) > 100
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: High request rate on web server
      description: '{{ $labels.instance }} has high request rate (>100 req/min) for the last 5 minutes.'
  - alert: HighCPULoad
    expr: node_load1 > 0.8
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: High CPU load detected
      description: '{{ $labels.instance }} has CPU load >0.8 for the last 5 minutes.'


